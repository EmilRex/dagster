---
title: "Using Dagster with Snowflake"
description: Store your Dagster assets in Snowflake
---

# Using Dagster with Snowflake

This guide focuses on how to store and load Dagster's [software-defined assets (SDAs)](/concepts/assets/software-defined-assets) in Snowflake.

## Prerequisites

- snowflake account
- credential info (account name, user name, password)
- pip install dagster-snowflake, dagster-snowflake-pandas

## I/O manager crash course

Dagster factors out reading and writing data to storage into [I/O Managers](/concepts/io-management/io-managers). This unlocks several benefits:

- When writing assets, you don't need to write additional code that writes the data to storage.
- The same logic for reading and writing data is used for all assets using the same I/O manager, meaning you assets will be written and read consistently.
- I/O managers can be swapped without changing the asset code, allowing you to write to different locations depending on if you are developing locally, or running in production

But how to I/O managers work?

<center>
  <Image
    alt="Diagram of assets and ops with an I/O manager"
    src="/images/concepts/io-managers.png"
    width={683}
    height={410}
  />
</center>

When running a job or materializing assets, whenever a value is returned from an op or asset, Dagster uses the I/O manager to store the value. Behind the scenes, Dagster passes the returned value to the `handle_output` method of the I/O manager, and the `handle_output` method takes care of storing the output correctly.

When an op or asset needs the value from an upstream op or asset as input, the I/O manager can also fetch the value from the location where it was stored by the `handle_output` method. Dagster calls the `load_input` method of the I/O manager with the information required to find where the value was stored and the I/O manager returns the correct value. The `handle_output` and `load_input` methods are symmetric, meaning that `handle_input(handle_output(X)) == X`.

For the Snowflake I/O manager, the `handle_output` and `load_input` methods take care of creating the SQL queries required to store and load data in Snowflake tables.

## Using the Snowflake IO manager

In the remainder of this guide, we will go over how to use the Snowflake I/O manager with your Dagster assets. Each of the following examples will go over how to achieve a particular task. The concepts covered can be combined together

### Example: Storing data in Snowflake

To store data in Snowflake using the Snowflake I/O manager, the definitions of your assets don't need to change. You can tell Dagster to use the Snowflake I/O Manager in your repository, and Dagster will handle storing and loading your assets in Snowflake.

```python file=/integrations/snowflake/basic_example.py
from dagster import asset, repository, with_resources
from dagster_snowflake_pandas import snowflake_pandas_io_manager
import pandas as pd

@asset
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )

@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured({
                "database": "FLOWERS",
                "schema": "IRIS",
                "account": "abc1234.us-east-1",
                "user": {
                    "env": "SNOWFLAKE_USER"
                },
                "password": {
                    "env": "SNOWFLAKE_PASSWORD"
                }
        })
        }
    )
```

Let's go over what's going on in this example. First we define our [asset](/concepts/assets/software-defined-assets). In this example we are fetching the Iris dataset as a Pandas DataFrame and renaming the columns. Because Dagster factors out storing and loading data into I/O managers, all we need to do is return the DataFrame and Dagster will manage storing the data. The type signature of the function tells the I/O manager what data type it is working with so it is important to include the return type `pd.DataFrame`.

However, we still need to tell Dagster that we want to store our data in Snowflake. We do this in the [repository](/concepts/repositories-workspaces/repositories). Since our asset returns a Pandas DataFrame, we need to tell Dagster to use a Snowflake I/O manager that can store and load Pandas DataFrames. This I/O manager is the `snowflake_pandas_io_manager` in the `dagster_snowflake_pandas` package.

<!-- uncomment when pyspark type handler is built -->

<!-- <Note>
Dagster has versions of the Snowflake I/O manager for multiple different dataframe types including Pandas. Additionally you can  create a Snowflake I/O manager that supports multiple types. See <a href="TODO LINK"> TODO SECTION NAME </a> for more information.
</Note> -->

We attach the `dagster_snowflake_pandas` I/O manager to our asset using the `with_resources` function. This function lets you supply the [resources](/concepts/resources) and I/O managers your assets need. In this case, we say that the `snowflake_pandas_io_manager` should be used as the `io_manager`. `io_manager` is a reserved key for supplying the I/O manager that should be used by default.

We also need to supply some configuration to the `snowflake_pandas_io_manager` so that it knows how to connect to your Snowflake instance. The configuration in this example tells the I/O manager to store outputs in the `FLOWERS` database under the `IRIS` schema. The table that is created for the data will be the name of the asset, in this case `iris_dataset`. We give the I/O manager the account information and tell it that our username and password can be found in the environment variables `SNOWFLAKE_USER` and `SNOWFLAKE_PASSWORD`, respectively.

When the `handle_output` method of the `snowflake_pandas_io_manager` will create the table `FLOWERS.IRIS.IRIS_DATASET` if it does not exist and replace the contents of the table with the value returned from the `iris_dataset` asset.

### Example: Making existing tables available in Dagster

You may already have tables in Snowflake that you want to make available to other Dagster assets. You can create [source assets](/concepts/assets/software-defined-assets#defining-external-asset-dependencies) for these tables.

```python file=/integrations/snowflake/source_asset.py
from dagster import SourceAsset, repository, with_resources
from dagster_snowflake_pandas import snowflake_pandas_io_manager


daffodil_dataset = SourceAsset(key="daffodil_dataset")

@repository
def flowers_analysis_repository():
    return with_resources(
        [daffodil_dataset],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured({
                "database": "FLOWERS",
                "schema": "DAFFODIL",
                "account": "abc1234.us-east-1",
                "user": {
                    "env": "SNOWFLAKE_USER"
                },
                "password": {
                    "env": "SNOWFLAKE_PASSWORD"
                }
        })
        }
    )
```

In this example, we create a `SourceAsset` for a pre-existing table containing data about daffodils (perhaps this table is created by an external data ingestion tool). In order to make the data available to other Dagster assets we need to tell the Snowflake I/O manager how to find the data. Since we are supplying the database and the schema in the I/O manager configuration, we only need to provide the table name. We do this with the `key` parameter in `SourceAsset`. When the I/O manager needs to load the `daffodil_dataset` in a downstream asset, it will select the data in the `FLOWERS.DAFFODIL.DAFFODIL_DATASET` table as a Pandas DataFrame and provide it to the downstream asset.

### Example: Loading Snowflake tables in downstream assets

Once you have created an asset or source asset that represents a table in Snowflake, you will likely need to create additional assets that work with the data. Dagster and the Snowflake I/O manager allow you to load the data stored in Snowflake tables into downstream assets

```python file=/integrations/snowflake/load_downstream.py
from dagster import asset, repository, with_resources
from dagster_snowflake_pandas import snowflake_pandas_io_manager
import pandas as pd


@asset
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )

@asset
def iris_cleaned(iris_dataset: pd.DataFrame):
    return iris_dataset.dropna().drop_duplicates()


@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset, iris_cleaned],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured({
                "database": "FLOWERS",
                "schema": "IRIS",
                "account": "abc1234.us-east-1",
                "user": {
                    "env": "SNOWFLAKE_USER"
                },
                "password": {
                    "env": "SNOWFLAKE_PASSWORD"
                }
        })
        }
    )
```

In this example, we create the `iris_dataset` as in the [Storing data in Snowflake](#example-storing-data-in-snowflake) example. In `iris_cleaned` the `iris_dataset` parameter tells Dagster that the value for the `iris_dataset` asset should be provided as input to `iris_cleaned` (if this feels too magical for you, see [this page](/concepts/assets/software-defined-assets#defining-explicit-dependencies) for how to explicitly specify dependencies). When materializing these assets, Dagster will use the `snowflake_pandas_io_manager` to fetch the `FLOWERS.IRIS.IRIS_DATASET` as a Pandas DataFrame and pass this DataFrame as the `iris_dataset` parameter to `iris_cleaned`. When `iris_cleaned` returns a Pandas DataFrame, Dagster will use the `snowflake_pandas_io_manager` to store the DataFrame as the `FLOWERS.IRIS.IRIS_CLEANED` table in Snowflake.

### Example: Selecting specific columns from a Snowflake table in downstream assets

Sometimes you may not want to fetch an entire table as the input to a downstream asset. In this case, you can select specific columns to load by supplying metadata on the downstream asset.

```python file=/integrations/snowflake/downstream_columns.py
from dagster import asset, repository, with_resources, AssetIn
from dagster_snowflake_pandas import snowflake_pandas_io_manager
import pandas as pd


@asset
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )

@asset(
    ins={"iris_sepal": AssetIn(
        key="iris_dataset",
        metadata={
            "columns": ["Sepal length (cm)", "Sepal width (cm)"]
        }
    )}

)
def sepal_data(iris_sepal: pd.DataFrame):
    iris_sepal["Sepal area (cm2)"] = iris_sepal["Sepal length (cm)"] * iris_sepal["Sepal width (cm)"]
    return iris_sepal


@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset, sepal_data],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured({
                "database": "FLOWERS",
                "schema": "IRIS",
                "account": "abc1234.us-east-1",
                "user": {
                    "env": "SNOWFLAKE_USER"
                },
                "password": {
                    "env": "SNOWFLAKE_PASSWORD"
                }
        })
        }
    )
```

In this example, we only need the columns with sepal data in our `sepal_data` asset. Fetching the entire table would be unnecessarily costly. To select specific columns, we need to add metadata to the input asset. We do this by explicitly specifying which asset should be loaded into our `sepal_data` asset. In the `ins` parameter to `sepal_data` we create a dictionary where the keys are the parameters for the asset function and the values are `AssetIn`s that tell Dagster what assets to load. In this example, we tell Dagster that the `iris_sepal` parameter should be the `iris_dataset` asset. In the `metadata` of the `AssetIn` we supply the key `columns` with a list of the column names we want to fetch. When Dagster materializes the `sepal_data` asset, when it loads the `iris_dataset` asset using the Snowflake I/O manager, it will only fetch the `Sepal length (cm)` and `Sepal width (cm)` columns of the `FLOWERS.IRIS.IRIS_DATASET` table as a Pandas DataFrame and pass them to `sepal_data`.

## Configuration

## Customizing the Schema

- go through the different ways to set the schema

## Partitions

- example showing how to create a partitioned asset in snowflake

## Combining with other IO Managers

- setting the io manager key on the asset and asset in

## Integrating with dbt

- how to store + load dbt assets in snowflake

## Supporting multiple types

- creating a snowflake io manager from scratch with multiple type handlers

## FAQ

- how does the Io manager work?

<!---->

- explain how we run the sql ourselves

<!---->

- how to run an arbitrary sql command?
